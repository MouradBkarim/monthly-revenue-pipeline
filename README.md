# üìä Monthly Customer Revenue Data Mart

This project demonstrates a full data stack workflow for building a **Snowflake-based data mart** with data quality and orchestration practices.

It uses:

‚úÖ **dbt** (transformations and tests)
‚úÖ **Soda** (data quality scans)
‚úÖ **Airflow (Astro)** (orchestration)
‚úÖ **Snowflake** (cloud data warehouse)

---

## üöÄ Use Case

> **Goal:** Build a data mart that tracks **Monthly Revenue per Customer by Region and Nation**.

It answers business questions like:

- _How much revenue did each customer generate per month?_
- _What is the breakdown by region and nation?_
- _How do discounts affect net revenue?_

---

## üìÇ Project Structure

```
.
‚îú‚îÄ‚îÄ .astro/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ dag.py
‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îî‚îÄ‚îÄ dbt/
‚îÇ       ‚îú‚îÄ‚îÄ cosmos_config.py
‚îÇ       ‚îú‚îÄ‚îÄ models/
‚îÇ           ‚îú‚îÄ‚îÄ staging/
‚îÇ           ‚îú‚îÄ‚îÄ intermediate/
‚îÇ           ‚îú‚îÄ‚îÄ marts/
‚îÇ       ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îú‚îÄ‚îÄ macros/
‚îÇ           ‚îú‚îÄ‚îÄ revenu.sql
‚îÇ       ‚îî‚îÄ‚îÄ seeds/
‚îÇ       ‚îî‚îÄ‚îÄ snapshots/
‚îÇ       ‚îú‚îÄ‚îÄ .......
‚îÇ   ‚îî‚îÄ‚îÄ soda/
‚îÇ       ‚îú‚îÄ‚îÄ configuration.yml
‚îÇ       ‚îú‚îÄ‚îÄ .....
‚îî‚îÄ‚îÄ .gitignore
```

### ‚ûú Staging Layer

- Clean, renamed, typed data from Snowflake `SNOWFLAKE_SAMPLE_DATA.TPCH_SF1`.
- Source freshness and relationship tests (dbt).

### ‚ûú Intermediate Layer

- Business logic joins:

  - Orders + LineItem ‚Üí Order Items with discount and revenue calcs.
  - Customers enriched with Nation and Region.

### ‚ûú Marts Layer

- Final fact table:

  - **Monthly revenue per customer, region, nation**.
  - With/without discount columns.

- Tests for accepted values, uniqueness, not nulls.

---

## üß™ Data Quality

### ‚úÖ dbt Tests

- Source-level tests (freshness, relationships).
- Model-level tests (accepted values, uniqueness, not nulls).

### ‚úÖ Soda Checks

- **Pre-transformation**: Validate source volumes and expected patterns.
- **Post-transformation**: Ensure transformed models meet business rules.

---

## ‚öôÔ∏è Orchestration

Orchestrated with **Airflow** using **Astro Runtime**.

- DAG runs:
  1Ô∏è‚É£ **Pre-load checks** (Soda scans on raw sources)
  2Ô∏è‚É£ **dbt source tests** (alternative approach)
  3Ô∏è‚É£ **Staging models**
  4Ô∏è‚É£ **Intermediate models**
  5Ô∏è‚É£ **Mart models + dbt tests**
  6Ô∏è‚É£ **Post-transform checks** (Soda scans on marts)

- Astro environment runs tasks in **virtualenvs** to support Soda, dbt-snowflake.

---

## üîó How to Run Locally

### 1Ô∏è‚É£ Prerequisites

- Snowflake trial account
- Soda trial account
- Astro CLI
- Python 3.11+

### 2Ô∏è‚É£ Clone and Set Up

```bash
git clone https://github.com/yourusername/monthly-customer-revenue
cd monthly-customer-revenue

# Copy and edit your .env
cp .env.example .env

# Init astro
astro dev init

```

> Store your Snowflake creds and Soda keys in .env.

### 3Ô∏è‚É£ Snowflake setup

Before running this project, make sure your Snowflake environment is prepared.

This repo includes a ready-to-run SQL script to set up the warehouse, database, role, and schema required for dbt.

‚û°Ô∏è Script location: **infrastructure/snowflake_setup.sql**

### 4Ô∏è‚É£ Start Astro

```bash
# compile
dbt compile --project-dir ./include/dbt --profiles-dir ./include/dbt

# run tests
dbt test --project-dir ./include/dbt --profiles-dir ./include/dbt --profile data_pipeline --target dev

# run Airflow
astro dev start --no-cache
```

### 5Ô∏è‚É£ Access Airflow

- Web UI: [http://localhost:8080](http://localhost:8080)
- Trigger the DAG: **monthly_customers_revenu_dag**

---

## ‚úÖ CI/CD with GitHub Actions

This project includes a **ready-to-use GitHub Actions workflow** that runs automated **CI checks** on every push and pull request.

---

### ‚öôÔ∏è What It Does

When you **push** to `main` or open a **Pull Request**, GitHub Actions will:

‚úÖ Install Python 3.11
‚úÖ Install `dbt-snowflake`
‚úÖ Install `soda-snowflake` (for consistency with local dev)
‚úÖ Install project dependencies (`dbt deps`)
‚úÖ Compile the dbt project (`dbt compile`)
‚úÖ Run all dbt tests (`dbt test`)

---

### üìÇ Workflow File

The workflow file lives at:

```
.github/workflows/dbt-ci.yml
```

---

üîê Secrets Management
This project does not include any real credentials in the repo.

‚úÖ You should add GitHub Actions secrets in your repository settings to provide your Snowflake and Soda credentials safely.

---

‚úÖ Recommended GitHub Actions secrets:

- `SNOWFLAKE_ACCOUNT`
- `SNOWFLAKE_USER`
- `SNOWFLAKE_PASSWORD`
- `SODA_CLOUD_HOST`
- `SODA_CLOUD_API_KEY_ID`
- `SODA_CLOUD_API_KEY_SECRET`
- `SOURCE_SNOWFLAKE_WAREHOUSE`
- `SOURCE_SNOWFLAKE_DATABASE`
- `SOURCE_SNOWFLAKE_SCHEMA`
- `SOURCE_SNOWFLAKE_ROLE`
- `DST_SNOWFLAKE_WAREHOUSE`
- `DST_SNOWFLAKE_DATABASE`
- `DST_SNOWFLAKE_SCHEMA`
- `DST_SNOWFLAKE_ROLE`

---

## üìú License

MIT License. Use and adapt freely.

---
